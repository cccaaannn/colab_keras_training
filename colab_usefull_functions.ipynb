{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_usefull_functions",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS4fm68nL5bpzUiSCET56f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cccaaannn/deep_learning_colab/blob/master/colab_usefull_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVAQ7iY9W6fj",
        "colab_type": "text"
      },
      "source": [
        "functions for using drive with colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-QUgnQaW_rP",
        "colab_type": "text"
      },
      "source": [
        "creating different size training data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBYMR017W5o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "import os\n",
        "index = 0\n",
        "for i,folder in enumerate(os.listdir(\"/content/drive/My Drive/Datalar100_2\")):\n",
        "  if(folder not in os.listdir(\"/content/drive/My Drive/data30\") and folder != \".DS_Store\"):\n",
        "    if(index<50):\n",
        "      index += 1\n",
        "      copy_tree(\"/content/drive/My Drive/Datalar100_2/\"+folder,\"/content/drive/My Drive/data50/\"+folder)\n",
        "      print(index)\n",
        "    else:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuVNPG67XHXI",
        "colab_type": "text"
      },
      "source": [
        "dividing training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dlnRB6mXKH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "from shutil import copyfile\n",
        "main_dir = \"/content/drive/My Drive/Datalar100_2\"\n",
        "\n",
        "folders = os.listdir(main_dir)\n",
        "folders.remove(\".DS_Store\")\n",
        "test_path = os.path.join(main_dir,\"test\")\n",
        "train_path = os.path.join(main_dir,\"train\")\n",
        "os.makedirs(test_path)\n",
        "os.makedirs(train_path)\n",
        "\n",
        "\n",
        "for i,folder in enumerate(folders):\n",
        "  try:\n",
        "    print(i)\n",
        "    current_test_folder = os.path.join(test_path,folder)\n",
        "    current_train_folder = os.path.join(train_path,folder)\n",
        "    os.makedirs(current_test_folder)\n",
        "    os.makedirs(current_train_folder)\n",
        "    for index, image in enumerate(os.listdir(os.path.join(main_dir,folder))):\n",
        "        current_img = os.path.join(main_dir,folder,image)\n",
        "        if(index<15):\n",
        "            copyfile(current_img,os.path.join(current_test_folder,image))\n",
        "        else:\n",
        "            copyfile(current_img,os.path.join(current_train_folder,image))\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wSToaKxZ58q",
        "colab_type": "text"
      },
      "source": [
        "**select tf 1.x**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2CCKSfZ5sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cdgmp1FyVd_",
        "colab_type": "text"
      },
      "source": [
        "**keras imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwMgo7r5yVFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# keras and tf\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# models\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "\n",
        "# backend\n",
        "from keras import optimizers, metrics, models\n",
        "import keras.backend as K\n",
        "\n",
        "# layers\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Activation\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D, BatchNormalization\n",
        "\n",
        "# optimizers\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# training\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
        "\n",
        "# save\n",
        "import h5py\n",
        "\n",
        "# keras aplications\n",
        "from keras.applications import DenseNet201, DenseNet169, InceptionResNetV2, ResNet152V2, InceptionV3, DenseNet121, Xception, MobileNet\n",
        "\n",
        "\n",
        "# other libs\n",
        "import math\n",
        "import pickle\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RVbXKvUHgAP",
        "colab_type": "text"
      },
      "source": [
        "keras impoers as tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKUyEBWvHevy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# keras and tf\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# backend\n",
        "from tensorflow.keras import optimizers, metrics, models\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# layers\n",
        "from tensorflow.keras.layers import Input, add, Conv2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D, BatchNormalization\n",
        "\n",
        "# optimizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adagrad, RMSprop\n",
        "\n",
        "# regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# training\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
        "\n",
        "# save\n",
        "import h5py\n",
        "\n",
        "# other libs\n",
        "import math\n",
        "import pickle\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmUE-rspHlhX",
        "colab_type": "text"
      },
      "source": [
        "keras aplications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY0t9S6_HlYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications import (DenseNet121,\n",
        " DenseNet169,\n",
        " DenseNet201,\n",
        " EfficientNetB0,\n",
        " EfficientNetB1,\n",
        " EfficientNetB2,\n",
        " EfficientNetB3,\n",
        " EfficientNetB4,\n",
        " EfficientNetB5,\n",
        " EfficientNetB6,\n",
        " EfficientNetB7,\n",
        " InceptionResNetV2,\n",
        " InceptionV3,\n",
        " MobileNet,\n",
        " MobileNetV2,\n",
        " NASNetLarge,\n",
        " NASNetMobile,\n",
        " ResNet101,\n",
        " ResNet101V2,\n",
        " ResNet152,\n",
        " ResNet152V2,\n",
        " ResNet50,\n",
        " ResNet50V2,\n",
        " VGG16,\n",
        " VGG19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKoMRxfdXtaH",
        "colab_type": "text"
      },
      "source": [
        "**keras image data generator**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osWYsDGJXwA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        # rotation -40 40\n",
        "        rotation_range=40,\n",
        "\n",
        "        # shift images and fill pixels with nearest\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "\n",
        "        shear_range=0.2,\n",
        "\n",
        "        brightness_range=[0.1, 0.5],\n",
        "        channel_shift_range=0.5,\n",
        "\n",
        "        # normalization \n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,\n",
        "        # validation_split=0.2,\n",
        "        )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "\n",
        "# train = train_datagen.flow_from_directory(\"/content/drive/My Drive/Data/train\", shuffle=True, class_mode='categorical', target_size=(image_size,image_size), batch_size=batch_size)\n",
        "# valid = valid_datagen.flow_from_directory(\"/content/drive/My Drive/Data/test\", shuffle=True, class_mode='categorical', target_size=(image_size,image_size), batch_size=batch_size)\n",
        "\n",
        "train = valid_datagen.flow_from_directory(\"/content/drive/My Drive/clean_data/train\", target_size=(image_size,image_size), batch_size=batch_size)\n",
        "valid = valid_datagen.flow_from_directory(\"/content/drive/My Drive/clean_data/test\", target_size=(image_size,image_size), batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErtcM43eHu-4",
        "colab_type": "text"
      },
      "source": [
        "keras image data generator v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsoHBBJPHuta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        # rotation -40 40\n",
        "        rotation_range=40,\n",
        "\n",
        "        # shift images and fill pixels with nearest\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.3,\n",
        "\n",
        "        shear_range=0.2,\n",
        "\n",
        "        brightness_range=[0.1, 0.5],\n",
        "        channel_shift_range=0.5,\n",
        "\n",
        "        # normalization \n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,\n",
        "        validation_split=0.2,\n",
        "        )\n",
        "\n",
        "train = train_datagen.flow_from_directory(\n",
        "    directory = all_dir,\n",
        "    target_size = (image_size,image_size), \n",
        "    batch_size = batch_size,\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "valid = train_datagen.flow_from_directory(\n",
        "    directory = all_dir,\n",
        "    target_size = (image_size,image_size), \n",
        "    batch_size = batch_size,\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-IIvNgAZ04N",
        "colab_type": "text"
      },
      "source": [
        "**convert model to sequential and add last layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSohYpCZ0d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "for layer in base_model.layers:\n",
        "    model.add(layer)\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(100, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRN0EF9tarPn",
        "colab_type": "text"
      },
      "source": [
        "**optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upMKNYkwaq1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "opt = SGD(lr=0.01, nesterov=True)\n",
        "opt = Adam(learning_rate=0.001)\n",
        "opt = Adagrad()\n",
        "opt = RMSprop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZzuuUSYdp17",
        "colab_type": "text"
      },
      "source": [
        "**top k accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YySoJb36dppM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
        "top3_acc.__name__ = 'top3_acc'\n",
        "\n",
        "top5_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=5)\n",
        "top5_acc.__name__ = 'top5_acc'\n",
        "\n",
        "top10_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=10)\n",
        "top10_acc.__name__ = 'top10_acc'\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy', top3_acc, top5_acc, top10_acc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS-I2hBIaHvJ",
        "colab_type": "text"
      },
      "source": [
        "**model checkpointer and learning schedular**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5IkQIFXaILu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = \"/content/drive/My Drive/model\"\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath= model_path+'-{epoch:02d}-{val_loss:0.5f}-{val_accuracy:0.5f}.hdf5', verbose=1, monitor='val_loss', save_best_only=True)\n",
        "\n",
        "def schedule(epoch):\n",
        "    if epoch < 15:\n",
        "        return 0.001\n",
        "    elif epoch < 20:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(schedule)\n",
        "\n",
        "history = model.fit_generator(train, epochs=100, verbose=1, validation_data=valid, callbacks=[lr_scheduler, checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAuUUrv3Tpkq",
        "colab_type": "text"
      },
      "source": [
        "download model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0aE-BZyTjv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/cats_and_dogs_small.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvWRqIf8-Cmf",
        "colab_type": "text"
      },
      "source": [
        "predict all test data and \n",
        "\n",
        "draw confusion matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfxUvvfN-ANo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_path = \"/content/drive/My Drive/Datasets/food_data_100/100/test\"\n",
        "test_classes = os.listdir(test_path)\n",
        "test_classes.sort()\n",
        "\n",
        "labels = []\n",
        "all_predictions = []\n",
        "for index, test_class in enumerate(test_classes):\n",
        "    predictions = make_prediction_from_directory_keras(os.path.join(test_path, test_class), model, model_summary=False, print_output=False)\n",
        "    for i in range(len(predictions)):\n",
        "        labels.append(index)\n",
        "    all_predictions += predictions\n",
        "    print(index)\n",
        "\n",
        "for x,y in zip(all_predictions, labels):\n",
        "    print(x,y)\n",
        "\n",
        "from  imagepreprocessing.utilities import create_confusion_matrix\n",
        "mat = create_confusion_matrix(all_predictions, labels, class_names=test_classes, one_hot=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckV7S0qU-wVD",
        "colab_type": "text"
      },
      "source": [
        "create lables for test directories create confusion matrix calculate accuricy from confusion matrix and save the matrix as csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTbXZebh-NLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_path = \"/content/drive/test\"\n",
        "\n",
        "\n",
        "labels = []\n",
        "\n",
        "counter = 0\n",
        "counter2 = 0\n",
        "for i in range(1500):\n",
        "  if(i %15==0 and i!=0):\n",
        "    counter += 1\n",
        "    labels.append(counter)\n",
        "    \n",
        "  else:\n",
        "    labels.append(counter)\n",
        "    \n",
        "import os \n",
        "class_names = os.listdir(images_path)\n",
        "class_names.sort()\n",
        "\n",
        "\n",
        "from  imagepreprocessing.utilities import create_confusion_matrix\n",
        "mat = create_confusion_matrix(all_predictions, labels, class_names=class_names, one_hot=False)\n",
        "\n",
        "\n",
        "def accuracy(confusion_matrix):\n",
        "    diagonal_sum = confusion_matrix.trace()\n",
        "    sum_of_all_elements = confusion_matrix.sum()\n",
        "    return diagonal_sum / sum_of_all_elements\n",
        "accuracy(mat)\n",
        "\n",
        "import numpy as np\n",
        "np.savetxt(\"mat.csv\", mat, delimiter=\",\", fmt=\"%d\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLEPuWBw_nhW",
        "colab_type": "text"
      },
      "source": [
        "make_prediction_with_predict_generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WKQa8J_iM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prediction_with_predict_generator(images_path, keras_model, image_size = (224,224), batch_size = 1, print_output = True, topn_acc = 5):\n",
        "    import numpy as np\n",
        "    import keras\n",
        "    from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "    # image_size parsing\n",
        "    if(type(image_size) == tuple):\n",
        "        if(len(image_size) == 2 and type(image_size[0]) == int and type(image_size[1]) == int):\n",
        "            img_width = image_size[0]\n",
        "            img_height = image_size[1]\n",
        "        else:\n",
        "            raise ValueError(\"image_size tuple should have 2 int values\")\n",
        "    elif(type(image_size) == int):\n",
        "        img_width = image_size\n",
        "        img_height = image_size\n",
        "    else:\n",
        "        raise ValueError(\"image_size should be an int or a tuple with 2 int values\")\n",
        "\n",
        "\n",
        "    # prepare model\n",
        "    if(isinstance(keras_model, keras.Model)):\n",
        "        model = keras_model\n",
        "    else:\n",
        "        model = keras.models.load_model(keras_model)\n",
        "\n",
        "\n",
        "    __test_generator = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "    test_flow = __test_generator.flow_from_directory(images_path, target_size=(img_width,img_height), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if(print_output):\n",
        "      print(test_flow.class_indices)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    all_preds = model.predict(test_flow)\n",
        "    for i, pred in enumerate(all_preds):\n",
        "        prediction = pred.argsort()[-topn_acc:][::-1]\n",
        "        predictions.append(prediction.tolist())\n",
        "        if(print_output):\n",
        "            print(i, prediction)\n",
        "    \n",
        "    \n",
        "    return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5l1T9-Emc9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prediction_with_predict_generator2(images_path, keras_model, image_size = (224,224), batch_size = 1, print_output = True, topn_acc = 5):\n",
        "    import numpy as np\n",
        "    import keras\n",
        "    import cv2\n",
        "    from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "    # image_size parsing\n",
        "    if(type(image_size) == tuple):\n",
        "        if(len(image_size) == 2 and type(image_size[0]) == int and type(image_size[1]) == int):\n",
        "            img_width = image_size[0]\n",
        "            img_height = image_size[1]\n",
        "        else:\n",
        "            raise ValueError(\"image_size tuple should have 2 int values\")\n",
        "    elif(type(image_size) == int):\n",
        "        img_width = image_size\n",
        "        img_height = image_size\n",
        "    else:\n",
        "        raise ValueError(\"image_size should be an int or a tuple with 2 int values\")\n",
        "\n",
        "\n",
        "    # prepare model\n",
        "    if(isinstance(keras_model, keras.Model)):\n",
        "        model = keras_model\n",
        "    else:\n",
        "        model = keras.models.load_model(keras_model)\n",
        "\n",
        "\n",
        "    __test_generator = ImageDataGenerator()\n",
        "    # test_flow = __test_generator.flow_from_directory(images_path, target_size=(img_width,img_height), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # img_array = cv2.imread(images_path)\n",
        "    # new_array = cv2.resize(img_array, (img_width, img_height))\n",
        "    # new_array = new_array.reshape(-1, img_width, img_height, 3)   \n",
        "    # test_flow = __test_generator.flow(new_array, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    image = keras.preprocessing.image.load_img(images_path, target_size=(img_width,img_height))\n",
        "    input_arr = keras.preprocessing.image.img_to_array(image)\n",
        "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
        "    # predictions = model.predict(input_arr)\n",
        "    __test_generator.fit(input_arr)\n",
        "    test_flow = __test_generator.flow(input_arr, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # if(print_output):\n",
        "    #   print(test_flow.class_indices)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    pred = model.predict(test_flow)\n",
        "    prediction = pred.argsort()[0][-topn_acc:][::-1]\n",
        "    predictions.append(prediction.tolist())\n",
        "    if(print_output):\n",
        "        print(prediction)\n",
        "\n",
        "    # all_preds = model.predict(test_flow)\n",
        "    # for i, pred in enumerate(all_preds):\n",
        "    #     prediction = pred.argsort()[-topn_acc:][::-1]\n",
        "    #     predictions.append(prediction.tolist())\n",
        "    #     if(print_output):\n",
        "    #         print(i, prediction)\n",
        "    \n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jyTbp0dmjNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = \"/content/drive/My Drive/models/densenet_100_77percent.h5\"\n",
        "import keras \n",
        "\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "import os\n",
        "dir = \"/content/drive/My Drive/Datasets/food_data_100/100/test/kestaneSekeri\"\n",
        "\n",
        "images = os.listdir(dir)\n",
        "images.sort()\n",
        "\n",
        "images = [os.path.join(dir, image) for image in images]\n",
        "\n",
        "\n",
        "for image in images:\n",
        "  make_prediction_with_predict_generator(image, model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}